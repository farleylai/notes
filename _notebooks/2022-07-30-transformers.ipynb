{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-07-30-transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Novel Applications using Transformers\n",
        "> Implication of memory mapped storage for super large data access.\n",
        "\n",
        "- hide: true\n",
        "- toc: false\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [blog, transformer, computer vision, deep learning, multimodal]"
      ],
      "metadata": {
        "id": "xBiIWit9fTB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TL; DR\n",
        "\n",
        "\n",
        "This post goes through transformer based architectures in various novel applications.\n",
        "\n",
        "- Transformer for End-to-End Object Detection - ðŸ”— Zhu et al. (2021)\n",
        "- Transformer for 3D Object Detection - ðŸ”— Bhattacharyya et al. (2021)\n",
        "- Transformer for Multi-Object Tracking - ðŸ”— Sun et al. (2020)\n",
        "- Transformer for Lane Shape Prediction - ðŸ”— Liu et al. (2020)\n",
        "- Transformer for Vision-Language Modeling - ðŸ”— Zhang et al. (2021)\n",
        "- Transformer for Image Synthesis - ðŸ”— Esser et al. (2020)\n",
        "- Transformer for Music Generation - ðŸ”— Hsiao et al. (2021)\n",
        "- Transformer for Dance Generation with Music - ðŸ”— Huang et al. (2021)\n",
        "- Transformer for Point-Cloud Processing - ðŸ”— Guo et al. (2020)\n",
        "- Transformer for Time-Series Forecasting - ðŸ”— Lim et al. (2020)"
      ],
      "metadata": {
        "id": "xbz6IUa-1749"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to training deep learning models, the I/O storage capacity and transfer bandwidth are ususally the bottleneck.\n",
        "While HDF5 is efficient to load the entire dataset for training, it is limited to the system memory capacity, typically up to hunderds of GB.\n",
        "On the other hand, memory mampped (MMAP) storage allows data access beyond system memory constraints.\n",
        "One popular implementation is LMDB, providing numerous language bindings including Python.\n",
        "It is tempting to replace HDF5 with LMDB for super large dataset loading and access.\n",
        "When running locally, potential memory allocation issues may not emerge to the surface as modern computer systems support disk swap space in case the process consumes more than available memory.\n",
        "However, training deep models could take days or longer and it is not uncommon to set up the training job in a high performance coomputing cluster such as SLURM.\n",
        "To submit a job to the SLURM cluster, the memory usage must be specified beforehand and at runtime, the memory usage is monitored according to the Resident Set Size (RSS) statistics.\n",
        "Unfortunately, the same training process on SLRUM is now subject to out of memory error because the swap space may not be available for SLURM tasks and MMAP based LMDB may grow the RSS over time beyond the usage limit. \n",
        "The following `pytest` snippet demonstrates the task RSS is increasing with LMDB access to a huage dataset over epochs:"
      ],
      "metadata": {
        "id": "nVWrtfItgu6z"
      }
    }
  ]
}