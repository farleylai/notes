{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-03-02-lmdb.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMW/NI7AfluzKqKSZtGsNEI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SLURM Out of Memory with LMDB\n",
        "> Implication of memory mapped storage for super large data access.\n",
        "\n",
        "- hide: false\n",
        "- toc: false\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [blog, slurm, lmdb, leak]"
      ],
      "metadata": {
        "id": "xBiIWit9fTB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TL; DR\t\n",
        "- SLURM monitors the total resident memory (RSS) consumed by all the task processes (incl. dataloader workers)\t\n",
        "- `pin_memory=True` increases RSS significantly and may cause leaks with mmap based LMDB, pushing to the memory limit sooner\n",
        "- PyTorch `FastDataLoader` or `DataLoader` created with `persistent_workers=True` is going to accumulate RSS with workers that never reset MMAP based storage such as `LMDB` env across epochs"
      ],
      "metadata": {
        "id": "xbz6IUa-1749"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to training deep learning models, the I/O storage capacity and transfer bandwidth are ususally the bottleneck.\n",
        "While HDF5 is efficient to load the entire dataset for training, it is limited to the system memory capacity, typically up to hunderds of GB.\n",
        "On the other hand, memory mampped (MMAP) storage allows data access beyond system memory constraints.\n",
        "One popular implementation is LMDB, providing numerous language bindings including Python.\n",
        "It is tempting to replace HDF5 with LMDB for super large dataset loading and access.\n",
        "When running locally, potential memory allocation issues may not emerge to the surface as modern computer systems support disk swap space in case the process consumes more than available memory.\n",
        "However, training deep models could take days or longer and it is not uncommon to set up the training job in a high performance coomputing cluster such as SLURM.\n",
        "To submit a job to the SLURM cluster, the memory usage must be specified beforehand and at runtime, the memory usage is monitored according to the Resident Set Size (RSS) statistics.\n",
        "Unfortunately, the same training process on SLRUM is now subject to out of memory error because the swap space may not be available for SLURM tasks and MMAP based LMDB may grow the RSS over time beyond the usage limit. \n",
        "The following `pytest` snippet demonstrates the task RSS is increasing with LMDB access to a huage dataset over epochs:"
      ],
      "metadata": {
        "id": "nVWrtfItgu6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print()\n",
        "\n",
        "KB = 2**10\n",
        "MB = 2**10 * KB\n",
        "GB = 2**10 * MB\n",
        "\n",
        "def rss_usage(breakdown=False):\n",
        "    import psutil\n",
        "    proc = psutil.Process(os.getpid())\n",
        "    RSS = []\n",
        "    RSS.append((os.getpid(), proc.memory_info().rss))\n",
        "    for child in proc.children(recursive=True):\n",
        "        RSS.append((child.pid, child.memory_info().rss))\n",
        "    \n",
        "    rss = sum(mem for _, mem in RSS)\n",
        "    return (rss, RSS) if breakdown else rss\n",
        "\n",
        "def test_rss():\n",
        "    print(sys.argv)\n",
        "    argv = sys.argv\n",
        "    sys.argv = [argv[1]]\n",
        "    import lmdb\n",
        "    from utils.utils import FastDataLoader\n",
        "    from dataset.lmdb_dataset import UCF101LMDB_2CLIP\n",
        "    from main_nce import parse_args, get_transform\n",
        "    args = parse_args()\n",
        "    sys.argv = argv\n",
        "    lmdb_root = \"/mnt/ssd/dataset/ucf101/lmdb\"\n",
        "    lmdb_path = f\"{lmdb_root}/UCF101/ucf101_frame.lmdb\"\n",
        "    trans = get_transform('train', args)\n",
        "    ucf101 = UCF101LMDB_2CLIP(db_path=lmdb_path, mode='train', transform=trans, num_frames=32, ds=1, return_label=True)\n",
        "    print(f\"Created UCF101 2clip dataset of size {len(ucf101)}\")\n",
        "\n",
        "    dataloader = FastDataLoader(ucf101, \n",
        "                            batch_size=32, shuffle=True,\n",
        "                            num_workers=4, persistent_workers=False, \n",
        "                            pin_memory=not True, sampler=None, drop_last=True)\n",
        "    batches = 8\n",
        "    for epoch in range(3):\n",
        "        rss = rss_usage()\n",
        "        print(f\"[e{epoch:02d}] RSS: {rss / GB:.2f} GB\")\n",
        "        for idx, (input_seq, label) in tqdm(enumerate(dataloader), total=len(dataloader), disable=True):\n",
        "            if idx % 4 == 0:\n",
        "                rss, RSS = rss_usage(True)\n",
        "                for pid, mem in RSS:\n",
        "                    print(f\"[e{epoch:02d}][b{idx:02d}][{pid}] consumes {mem / GB:.2f} GB\")\n",
        "                print(f\"[e{epoch:02d}][b{idx:02d}] RSS: {rss / GB:.2f} GB\")\n",
        "            if idx == batches:\n",
        "                break"
      ],
      "metadata": {
        "id": "DZvUrWjofNHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[e00] RSS: 2.56 GB\n",
        "[e00][b00][14023] consumes 1.08 GB\n",
        "[e00][b00][14055] consumes 0.49 GB\n",
        "[e00][b00][14071] consumes 0.81 GB\n",
        "[e00][b00][14087] consumes 0.83 GB\n",
        "[e00][b00][14103] consumes 0.84 GB\n",
        "[e00][b00] RSS: 4.07 GB\n",
        "[e00][b04][14023] consumes 1.08 GB\n",
        "[e00][b04][14055] consumes 0.78 GB\n",
        "[e00][b04][14071] consumes 0.90 GB\n",
        "[e00][b04][14087] consumes 0.64 GB\n",
        "[e00][b04][14103] consumes 0.80 GB\n",
        "[e00][b04] RSS: 4.20 GB\n",
        "[e00][b08][14023] consumes 1.08 GB\n",
        "[e00][b08][14055] consumes 0.97 GB\n",
        "[e00][b08][14071] consumes 1.00 GB\n",
        "[e00][b08][14087] consumes 0.66 GB\n",
        "[e00][b08][14103] consumes 1.24 GB\n",
        "[e00][b08] RSS: 4.95 GB\n",
        "[e01] RSS: 4.97 GB\n",
        "[e01][b00][14023] consumes 1.08 GB\n",
        "[e01][b00][14055] consumes 0.66 GB\n",
        "[e01][b00][14071] consumes 0.92 GB\n",
        "[e01][b00][14087] consumes 0.66 GB\n",
        "[e01][b00][14103] consumes 0.80 GB\n",
        "[e01][b00] RSS: 4.12 GB\n",
        "[e01][b04][14023] consumes 1.08 GB\n",
        "[e01][b04][14055] consumes 0.80 GB\n",
        "[e01][b04][14071] consumes 0.99 GB\n",
        "[e01][b04][14087] consumes 1.04 GB\n",
        "[e01][b04][14103] consumes 1.03 GB\n",
        "[e01][b04] RSS: 4.93 GB\n",
        "[e01][b08][14023] consumes 1.08 GB\n",
        "[e01][b08][14055] consumes 0.87 GB\n",
        "[e01][b08][14071] consumes 1.05 GB\n",
        "[e01][b08][14087] consumes 1.19 GB\n",
        "[e01][b08][14103] consumes 1.07 GB\n",
        "[e01][b08] RSS: 5.26 GB\n",
        "[e02] RSS: 5.29 GB\n",
        "[e02][b00][14023] consumes 1.08 GB\n",
        "[e02][b00][14055] consumes 0.85 GB\n",
        "[e02][b00][14071] consumes 1.06 GB\n",
        "[e02][b00][14087] consumes 1.09 GB\n",
        "[e02][b00][14103] consumes 1.09 GB\n",
        "[e02][b00] RSS: 5.17 GB\n",
        "[e02][b04][14023] consumes 1.08 GB\n",
        "[e02][b04][14055] consumes 0.92 GB\n",
        "[e02][b04][14071] consumes 1.12 GB\n",
        "[e02][b04][14087] consumes 0.86 GB\n",
        "[e02][b04][14103] consumes 1.14 GB\n",
        "[e02][b04] RSS: 5.12 GB\n",
        "[e02][b08][14023] consumes 1.08 GB\n",
        "[e02][b08][14055] consumes 0.97 GB\n",
        "[e02][b08][14071] consumes 1.19 GB\n",
        "[e02][b08][14087] consumes 0.93 GB\n",
        "[e02][b08][14103] consumes 1.23 GB\n",
        "[e02][b08] RSS: 5.39 GB"
      ],
      "metadata": {
        "id": "EM7WJme3qSgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The root cause is the `LMDB` Python API to access database records as follows may not release the mapped memory timely on completion to reduce the runtime RSS."
      ],
      "metadata": {
        "id": "7wVw3tkF_Jro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UCF101LMDB_2CLIP(object):\n",
        "        ...\n",
        "        print('Loading LMDB from %s, split:%d' % (self.db_path, self.which_split))\n",
        "        self.env = lmdb.open(self.db_path, subdir=os.path.isdir(self.db_path),\n",
        "                             readonly=True, lock=False,\n",
        "                             readahead=False, meminit=False)\n",
        "        ...\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        vpath, vlen, vlabel, vname = self.video_subset.iloc[index]\n",
        "        env = self.env\n",
        "        with env.begin(write=False) as txn:\n",
        "            raw = msgpack.loads(txn.get(self.get_video_id[vname].encode('ascii')))"
      ],
      "metadata": {
        "id": "tZIeNR97_4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Worse, the [FastLoader](https://bit.ly/3vvTXtj) never recreates dataset iterator workers that involes the `LMDB` env and will grow RSS over epochs due to increasing MMAP access.\n",
        "If using the vanilla `DataLoader`, make sure to set `persistent_workers=False` in case of a similar memory leak.\n",
        "Nonetheless, sufficient memory must be allocated at least for peak usage in one epoch.\n",
        "This serves as the workaround."
      ],
      "metadata": {
        "id": "9DiPzglnAA7j"
      }
    }
  ]
}